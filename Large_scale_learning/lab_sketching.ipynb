{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IA317: Large-scale machine learning\n",
    "# Sketching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn to work with [Min Hash](https://en.wikipedia.org/wiki/MinHash), a simple and efficient sketching algorithm to get approximate nearest neighbors for binary (sparse) data. You will find below some functions to build hashing tables and to find approximate $k$-nearest neighbors using Min Hash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please provide short answers to the questions at the bottom of the notebook. Most involve Python coding. Add as many cells as necessary (code and text). \n",
    "\n",
    "This lab is not graded but you might upload it on [eCampus](https://ecampus.paris-saclay.fr/course/view.php?id=18426) if you wish. Before that, make sure to:\n",
    "* Delete all useless cells (tests, etc.)\n",
    "* Check that **your code is running and does not produce any errors**. You might restart the kernel and run all cells at the end of the lab to check that this is indeed the case. \n",
    "* Keep the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab is based on the [20newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = dataset_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = dataset_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "print(target_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19]),\n",
       " array([480, 584, 591, 590, 578, 593, 585, 594, 598, 597, 600, 595, 591,\n",
       "        594, 593, 599, 546, 564, 465, 377]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = dataset_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is vectorized and binarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.2, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer = CountVectorizer(min_df=5, max_df=0.2, stop_words='english')\n",
    "tf_vectorizer.fit(dataset_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf_vectorizer.transform(dataset_train.data)\n",
    "X_test = tf_vectorizer.transform(dataset_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.data = np.ones(len(X_train.data))\n",
    "X_test.data = np.ones(len(X_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 25614)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7532, 25614)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min Hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some useful functions for Min Hash sketching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x, X):\n",
    "    '''Get Jaccard similarities between a target and a set of samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Vector of size d.\n",
    "    X : np.ndarray or sparse csr matrix.\n",
    "        Data, as array of shape (n,d).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sims : np.ndarray\n",
    "        Jaccard similarities, as a vector of size n.\n",
    "    '''\n",
    "\n",
    "    sims = np.zeros(X.shape[0])\n",
    "    \n",
    "    for i in range (X.shape[0]):\n",
    "        sims[i]= (np.sum(X[i]*x))/(np.sum(X[i])+np.sum(x)-np.sum(X[i]*x))\n",
    "\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permutations(d, L = 100):\n",
    "    '''Get permutations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d : int\n",
    "        Dimension (number of indices to shuffle).\n",
    "    L : int\n",
    "        Number of permutations.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Permutations : np.ndarray\n",
    "        Permutations as array of shape (L,d)\n",
    "    '''\n",
    "    permutations = []\n",
    "    for l in range(L):\n",
    "        index = np.arange(d)\n",
    "        np.random.shuffle(index)    \n",
    "        permutations.append(list(index))\n",
    "    return np.array(permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature(X, permutations):\n",
    "    '''Compute the MinHash of each sample.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : sparse csr matrix.\n",
    "        Data (binary features), shape (n, d).\n",
    "    permutations : np.ndarray\n",
    "        Permutations as array of shape (L,d)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    signature : np.ndarray\n",
    "        MinHash signature as array of shape (n, L)\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    L = permutations.shape[0]\n",
    "    signatures = np.zeros((n,L))\n",
    "    \n",
    "    for i in range (L):\n",
    "        for j in range (n):\n",
    "            k = 0\n",
    "            while X[j,permutations[i,k]] == 0 :\n",
    "                k += 1\n",
    "            signatures[j,i] = permutations[i,k]\n",
    "\n",
    "    return signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hash_tables(signature):\n",
    "    '''Build hash tables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signature : np.ndarray\n",
    "        Data signature as array of shape (n, L)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    hash_tables : list of dict\n",
    "        List of L hash tables\n",
    "    '''    \n",
    "    hash_tables = []\n",
    "    for sig in signature.T:\n",
    "        hash_tables.append({s: list(np.argwhere(sig == s).ravel()) for s in np.unique(sig)})\n",
    "    return hash_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_approximate_knn(signature_test, hash_tables, X_train, X_test, k = 3, factor = 10):\n",
    "    '''Get approximate k-nearest neighbors (for Jaccard distance).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signature_test : np.ndarray\n",
    "        Data signature as array of shape (n_test, L).\n",
    "    hash_tables : list of dict\n",
    "        List of L hash tables (based on train set).\n",
    "    X_train : np.ndarray or sparse csr matrix\n",
    "        Training data, shape (n_train, d).\n",
    "    X_test : np.ndarray or sparse csr matrix\n",
    "        Test data, shape (n_test, d)\n",
    "    k : int\n",
    "        Number of nearest neighbors.\n",
    "    factor : int\n",
    "        Multiplicative factor. \n",
    "        Nearest neighbors are searched in a list of factor * k samples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nn : np.ndarray\n",
    "        Approximate k-nearest neighbors, as arrays of shape (n_test, k).\n",
    "        Each entry is in range(n_train)\n",
    "    '''    \n",
    "    nn_list = []\n",
    "    for i, sig in enumerate(signature_test):\n",
    "        # search potential nearest neighbors\n",
    "        neighbors = []\n",
    "        for j, key in enumerate(sig):\n",
    "            if key in hash_tables[j]:\n",
    "                neighbors += hash_tables[j][key]\n",
    "        values, counts = np.unique(neighbors, return_counts = True) \n",
    "        # compute actual nearest neighbors\n",
    "        if len(values) >= k:\n",
    "            indices = values[np.argsort(-counts)][:factor * k]\n",
    "            unit_vector = np.zeros(X_test.shape[0])\n",
    "            unit_vector[i] = 1\n",
    "            x_test = X_test.T.dot(unit_vector)\n",
    "            nn_list.append(indices[np.argsort(-jaccard_similarity(x_test, X_train[indices]))[:k]])\n",
    "        else:\n",
    "            # complete with random values if necessary\n",
    "            nn_list.append(np.array(list(values) + list(np.random.choice(X_train.shape[0], size = k - len(values)))))\n",
    "    return np.array(nn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(nn, y_train):\n",
    "    '''Classification based on list of k-nearest neighbors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    nn_list : np.ndarray\n",
    "        k-nearest neighbors, as arrays of shape (n_test, k).\n",
    "        Each entry is in range(n_train)\n",
    "    y_train : np.ndarray\n",
    "        Target labels of the train set, array of shape (n_train,).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : np.ndarray\n",
    "        Predicted labels of the test set, array of shape (n_test,).\n",
    "    '''    \n",
    "    y_pred = []\n",
    "    for nn_ in nn:\n",
    "        labels, counts = np.unique(y_train[nn_], return_counts=True)\n",
    "        y_pred.append(labels[np.argmax(counts)])\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless otherwise specified, the considered metric is the [Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Complete the ``jaccard_similarity`` function. Make sure that your code does not produce any dense matrix.<br>\n",
    "What is the nearest neighbor of the following sentence in the training set? What is the corresponding newsgroup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Ice hockey is a team sport played on ice, in which two teams of skaters use their sticks to shoot a puck into their opponent's net to score points.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ice hockey is a team sport played on ice, in which two teams of skaters use their sticks to shoot a puck into their opponent's net to score points.\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse matrix\n",
    "X_sample = tf_vectorizer.transform([sentence])\n",
    "X_sample.data = (X_sample.data > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense vector\n",
    "x_sample = np.array(X_sample.todense()).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_max_sim = np.argmax(jaccard_similarity(x_sample, X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: chuck@mks.com (Chuck Lownie)\n",
      "Subject: Re: Tie Breaker....(Isles and Devils)\n",
      "Organization: Mortice Kern Systems Inc., Waterloo, Ontario, CANADA\n",
      "Lines: 27\n",
      "\n",
      "In article <lrw509f@rpi.edu> wangr@rpi.edu writes:\n",
      ">\tAre people here stupid or what??? It is a tie breaker, of cause they\n",
      ">have to have the same record. How can people be sooooo stuppid to put win as\n",
      ">first in the list for tie breaker??? If it is a tie breaker, how can there be\n",
      ">different record???? Man, I thought people in this net are good with hockey.\n",
      ">I might not be great in Math, but tell me how can two teams ahve the same points\n",
      ">with different record??? Man...retard!!!!!! Can't believe people actually put\n",
      ">win as first in a tie breaker......\n",
      ">\n",
      ">\n",
      "\n",
      "\n",
      "I didn't see any smilies in this message so.......\n",
      "\n",
      "                W     T    L    PTs\n",
      "   Team A      50    30    4    104\n",
      "   Team B      52    32    0    104\n",
      "\n",
      "\n",
      "There you go.  Two teams that tie in points without identical records.\n",
      "\n",
      "\n",
      "-- \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.data[id_max_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "print(target_names[y_train[id_max_sim]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the common words between the above sentence and its nearest neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = tf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_sentence = sentence.split()\n",
    "words_example = dataset_train.data[id_max_sim].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1=[]\n",
    "for i in words_sentence:\n",
    "    if i in feat_dict:\n",
    "        words_1.append(i)\n",
    "\n",
    "words_2=[]\n",
    "for i in words_example:\n",
    "    if i in feat_dict:\n",
    "        words_2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common words are: ['is', 'a', 'in', 'two', 'teams', 'of', 'to', 'a', 'net', 'to']\n"
     ]
    }
   ],
   "source": [
    "common = []\n",
    "for i in range (len(words_sentence)):\n",
    "    if words_sentence[i] in words_example:\n",
    "        common.append(words_sentence[i])\n",
    "print('Common words are:' , common)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Complete the function ``get_signature``.<br>\n",
    "Get 3 nearest neighbors of the above sentence using Min Hash with 100 permutations.<br>\n",
    "Display the corresponding newsgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = X_train.shape[1]\n",
    "permutations = get_permutations(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 25614)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permutations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_train = get_signature(X_train, permutations)\n",
    "hash_tables = get_hash_tables(signature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_sample = get_signature(X_sample, permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = get_approximate_knn(signature_sample, hash_tables, X_train, X_sample, k = 3, factor = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_classifier(nn, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding news group: rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "print('The corresponding news group:', target_names[y_pred[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the accuracy of (approximate) 3-nn classification using Min Hash with 100 permutations?<br>\n",
    "Compare with the exact 3-nn classification based on (a) the Hamming distance, (b) the cosine similarity after SVD.<br>\n",
    "Comment the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_test = get_signature(X_test, permutations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = get_approximate_knn(signature_test, hash_tables, X_train, X_test, k = 3, factor = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn_classifier(nn, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6456452469463622"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=50, n_iter=10, random_state=37)\n",
    "X_train_reduced = svd.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0523101433882103"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3,  metric = 'hamming')\n",
    "neigh.fit(X_train_reduced, y_train)\n",
    "X_test_reduced = svd.transform(X_test)\n",
    "y_pred = neigh.predict(X_test_reduced)\n",
    "accuracy_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5075677110993096"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = KNeighborsClassifier(n_neighbors=3,  metric = 'cosine')\n",
    "neigh.fit(X_train_reduced, y_train)\n",
    "X_test_reduced = svd.transform(X_test)\n",
    "y_pred = neigh.predict(X_test_reduced)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is 10 times better with cosine similarity but still far away from what we get using mini hash algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
