{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A (very small) introduction to pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Tensors are very similar to Numpy arrays, with the added benefit of being usable on GPU. For a short tutorial on various methods to create tensors of particular types, see [this link](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py).\n",
    "The important things to note are that Tensors can be created empty, from lists, and it is very easy to convert a numpy array into a pytorch tensor, and inversely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.LongTensor(5)\n",
    "b = torch.LongTensor([5])\n",
    "\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([                  0, 7089621706657183540, 7089007972926448695,\n",
      "        3688558274541610034, 7077183838608451174])\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([2])\n",
    "b = torch.FloatTensor([3])\n",
    "\n",
    "print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interest in us using Pytorch is the ```autograd``` package. ```torch.Tensor```objects have an attribute ```.requires_grad```; if set as True, it starts to track all operations on it. When you finish your computation, can call ```.backward()``` and all the gradients are computed automatically (and stored in the ```.grad``` attribute).\n",
    "\n",
    "One way to easily cut a tensor from the computational once it is not needed anymore is to use ```.detach()```.\n",
    "More info on automatic differentiation in pytorch on [this link](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "# Build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "# Compute gradients.\n",
    "y.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print(x.grad)    # x.grad = 2 \n",
    "print(w.grad)    # w.grad = 1 \n",
    "print(b.grad)    # b.grad = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight\n",
      "Parameter containing:\n",
      "tensor([[-0.1583, -0.1521,  0.5613],\n",
      "        [ 0.1849, -0.5349, -0.3317]], requires_grad=True)\n",
      "bias\n",
      "Parameter containing:\n",
      "tensor([-0.0956, -0.5487], requires_grad=True)\n",
      "Initial loss:  1.9882283210754395\n",
      "dL/dw:  tensor([[ 0.0466, -0.2819,  1.0870],\n",
      "        [ 0.3121,  0.1475, -0.2172]])\n",
      "dL/db:  tensor([-0.1086, -0.5911])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(10, 3)\n",
    "y = torch.randn(10, 2)\n",
    "\n",
    "# Build a fully connected layer.\n",
    "linear = nn.Linear(3, 2)\n",
    "for name, p in linear.named_parameters():\n",
    "    print(name)\n",
    "    print(p)\n",
    "\n",
    "# Build loss function - Mean Square Error\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Forward pass.\n",
    "pred = linear(x)\n",
    "\n",
    "# Compute loss.\n",
    "loss = criterion(pred, y)\n",
    "print('Initial loss: ', loss.item())\n",
    "\n",
    "# Backward pass.\n",
    "loss.backward()\n",
    "\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after one update:  1.9704128503799438\n"
     ]
    }
   ],
   "source": [
    "# You can perform gradient descent manually, with an in-place update ...\n",
    "linear.weight.data.sub_(0.01 * linear.weight.grad.data)\n",
    "linear.bias.data.sub_(0.01 * linear.bias.grad.data)\n",
    "\n",
    "# Print out the loss after 1-step gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('Loss after one update: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after two updates:  1.9529603719711304\n"
     ]
    }
   ],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of the model.\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# By default, gradients are accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "# is called. Before the backward pass, we need to use the optimizer object to zero all of the\n",
    "# gradients.\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Calling the step function on an Optimizer makes an update to its parameters\n",
    "optimizer.step()\n",
    "\n",
    "# Print out the loss after the second step of gradient descent.\n",
    "pred = linear(x)\n",
    "loss = criterion(pred, y)\n",
    "print('Loss after two updates: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a ```Dictionary``` class, that we are going to use to create a vocabulary for our text data. The goal here is to have a convenient tool, with easy access to any information we could need:\n",
    "- A python dictionary ```word2idx``` allowing easy transformation of tokenized text into indexes\n",
    "- A list ```idx2word```, allowing us to find the word corresponding to an index (for interpretation and generation)\n",
    "- A python dictionary ```counter``` used to build the vocabulary, that can provide us with frequency information if needed. \n",
    "- The ```total``` count of words in the dictionary.\n",
    "\n",
    "Important: The data that we are going to use are already pre-processed so we don't need to create special tokens and control the size of the vocabulary ourselves. However, when the text data is raw, methods to preprocess it conveniently should be added here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.counter = {}\n",
    "        self.total = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "            self.counter.setdefault(word, 0)\n",
    "        self.counter[word] += 1\n",
    "        self.total += 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      " = Valkyria Chronicles III = \n",
      "\n",
      " \n",
      "\n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./wikitext-2/train.txt', 'r') as f:\n",
    "    print(f.readline())\n",
    "    print(f.readline())\n",
    "    print(f.readline())\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\"': 60,\n",
      " '(': 9,\n",
      " ')': 18,\n",
      " ',': 12,\n",
      " '.': 14,\n",
      " '2011': 44,\n",
      " '3': 6,\n",
      " ':': 7,\n",
      " '<unk>': 8,\n",
      " '=': 0,\n",
      " '@-@': 29,\n",
      " 'Battlefield': 17,\n",
      " 'Chronicles': 2,\n",
      " 'Europan': 70,\n",
      " 'Gallia': 67,\n",
      " 'III': 3,\n",
      " 'Imperial': 80,\n",
      " 'January': 43,\n",
      " 'Japan': 24,\n",
      " 'Japanese': 10,\n",
      " 'Media.Vision': 37,\n",
      " 'Nameless': 61,\n",
      " 'PlayStation': 39,\n",
      " 'Portable': 40,\n",
      " 'Raven': 81,\n",
      " 'Released': 41,\n",
      " 'Second': 69,\n",
      " 'Sega': 35,\n",
      " 'Senjō': 4,\n",
      " 'Valkyria': 1,\n",
      " 'War': 71,\n",
      " 'a': 26,\n",
      " 'against': 79,\n",
      " 'and': 36,\n",
      " 'are': 77,\n",
      " 'as': 22,\n",
      " 'black': 75,\n",
      " 'by': 34,\n",
      " 'commonly': 19,\n",
      " 'developed': 33,\n",
      " 'during': 68,\n",
      " 'first': 58,\n",
      " 'follows': 59,\n",
      " 'for': 38,\n",
      " 'fusion': 49,\n",
      " 'game': 32,\n",
      " 'gameplay': 52,\n",
      " 'in': 42,\n",
      " 'is': 25,\n",
      " 'it': 45,\n",
      " 'its': 53,\n",
      " 'lit': 13,\n",
      " 'military': 63,\n",
      " 'nation': 66,\n",
      " 'no': 5,\n",
      " 'of': 15,\n",
      " 'operations': 76,\n",
      " 'outside': 23,\n",
      " 'parallel': 57,\n",
      " 'penal': 62,\n",
      " 'perform': 73,\n",
      " 'pitted': 78,\n",
      " 'playing': 30,\n",
      " 'predecessors': 54,\n",
      " 'real': 50,\n",
      " 'referred': 20,\n",
      " 'role': 28,\n",
      " 'runs': 56,\n",
      " 'same': 48,\n",
      " 'secret': 74,\n",
      " 'series': 47,\n",
      " 'serving': 65,\n",
      " 'story': 55,\n",
      " 'tactical': 27,\n",
      " 'the': 16,\n",
      " 'third': 46,\n",
      " 'time': 51,\n",
      " 'to': 21,\n",
      " 'unit': 64,\n",
      " 'video': 31,\n",
      " 'who': 72,\n",
      " '戦場のヴァルキュリア3': 11}\n",
      "['=',\n",
      " 'Valkyria',\n",
      " 'Chronicles',\n",
      " 'III',\n",
      " 'Senjō',\n",
      " 'no',\n",
      " '3',\n",
      " ':',\n",
      " '<unk>',\n",
      " '(',\n",
      " 'Japanese',\n",
      " '戦場のヴァルキュリア3',\n",
      " ',',\n",
      " 'lit',\n",
      " '.',\n",
      " 'of',\n",
      " 'the',\n",
      " 'Battlefield',\n",
      " ')',\n",
      " 'commonly',\n",
      " 'referred',\n",
      " 'to',\n",
      " 'as',\n",
      " 'outside',\n",
      " 'Japan',\n",
      " 'is',\n",
      " 'a',\n",
      " 'tactical',\n",
      " 'role',\n",
      " '@-@',\n",
      " 'playing',\n",
      " 'video',\n",
      " 'game',\n",
      " 'developed',\n",
      " 'by',\n",
      " 'Sega',\n",
      " 'and',\n",
      " 'Media.Vision',\n",
      " 'for',\n",
      " 'PlayStation',\n",
      " 'Portable',\n",
      " 'Released',\n",
      " 'in',\n",
      " 'January',\n",
      " '2011',\n",
      " 'it',\n",
      " 'third',\n",
      " 'series',\n",
      " 'same',\n",
      " 'fusion',\n",
      " 'real',\n",
      " 'time',\n",
      " 'gameplay',\n",
      " 'its',\n",
      " 'predecessors',\n",
      " 'story',\n",
      " 'runs',\n",
      " 'parallel',\n",
      " 'first',\n",
      " 'follows',\n",
      " '\"',\n",
      " 'Nameless',\n",
      " 'penal',\n",
      " 'military',\n",
      " 'unit',\n",
      " 'serving',\n",
      " 'nation',\n",
      " 'Gallia',\n",
      " 'during',\n",
      " 'Second',\n",
      " 'Europan',\n",
      " 'War',\n",
      " 'who',\n",
      " 'perform',\n",
      " 'secret',\n",
      " 'black',\n",
      " 'operations',\n",
      " 'are',\n",
      " 'pitted',\n",
      " 'against',\n",
      " 'Imperial',\n",
      " 'Raven']\n",
      "{'\"': 4,\n",
      " '(': 1,\n",
      " ')': 1,\n",
      " ',': 6,\n",
      " '.': 4,\n",
      " '2011': 1,\n",
      " '3': 2,\n",
      " ':': 2,\n",
      " '<unk>': 3,\n",
      " '=': 2,\n",
      " '@-@': 2,\n",
      " 'Battlefield': 1,\n",
      " 'Chronicles': 3,\n",
      " 'Europan': 1,\n",
      " 'Gallia': 1,\n",
      " 'III': 2,\n",
      " 'Imperial': 1,\n",
      " 'January': 1,\n",
      " 'Japan': 2,\n",
      " 'Japanese': 1,\n",
      " 'Media.Vision': 1,\n",
      " 'Nameless': 1,\n",
      " 'PlayStation': 1,\n",
      " 'Portable': 1,\n",
      " 'Raven': 1,\n",
      " 'Released': 1,\n",
      " 'Second': 1,\n",
      " 'Sega': 1,\n",
      " 'Senjō': 1,\n",
      " 'Valkyria': 5,\n",
      " 'War': 1,\n",
      " 'a': 2,\n",
      " 'against': 1,\n",
      " 'and': 4,\n",
      " 'are': 1,\n",
      " 'as': 2,\n",
      " 'black': 1,\n",
      " 'by': 1,\n",
      " 'commonly': 1,\n",
      " 'developed': 1,\n",
      " 'during': 1,\n",
      " 'first': 1,\n",
      " 'follows': 1,\n",
      " 'for': 1,\n",
      " 'fusion': 1,\n",
      " 'game': 3,\n",
      " 'gameplay': 1,\n",
      " 'in': 3,\n",
      " 'is': 2,\n",
      " 'it': 1,\n",
      " 'its': 1,\n",
      " 'lit': 1,\n",
      " 'military': 1,\n",
      " 'nation': 1,\n",
      " 'no': 1,\n",
      " 'of': 3,\n",
      " 'operations': 1,\n",
      " 'outside': 1,\n",
      " 'parallel': 1,\n",
      " 'penal': 1,\n",
      " 'perform': 1,\n",
      " 'pitted': 1,\n",
      " 'playing': 1,\n",
      " 'predecessors': 1,\n",
      " 'real': 1,\n",
      " 'referred': 1,\n",
      " 'role': 1,\n",
      " 'runs': 1,\n",
      " 'same': 1,\n",
      " 'secret': 1,\n",
      " 'series': 1,\n",
      " 'serving': 1,\n",
      " 'story': 1,\n",
      " 'tactical': 2,\n",
      " 'the': 11,\n",
      " 'third': 1,\n",
      " 'time': 1,\n",
      " 'to': 2,\n",
      " 'unit': 2,\n",
      " 'video': 1,\n",
      " 'who': 1,\n",
      " '戦場のヴァルキュリア3': 1}\n",
      "132\n"
     ]
    }
   ],
   "source": [
    "# Let's take the four first lines of our training data:\n",
    "corpus = ''\n",
    "with open('./wikitext-2/train.txt', 'r') as f:\n",
    "    for i in range(4):\n",
    "        corpus += f.readline()\n",
    "        \n",
    "# Create an empty Dictionary, separate and add all words. \n",
    "dictio = Dictionary()\n",
    "words = corpus.split()\n",
    "for word in words:\n",
    "    dictio.add_word(word)\n",
    "\n",
    "# Take a look at the objects created:\n",
    "pp.pprint(dictio.word2idx)\n",
    "pp.pprint(dictio.idx2word)\n",
    "pp.pprint(dictio.counter)\n",
    "pp.pprint(dictio.total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        # We create an object Dictionary associated to Corpus\n",
    "        self.dictionary = Dictionary()\n",
    "        # We go through all files, adding all words to the dictionary\n",
    "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
    "        \n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file, knowing the dictionary, in order to tranform it into a list of indexes\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "                tokens += len(words)\n",
    "        \n",
    "        # Once done, go through the file a second time and fill a Torch Tensor with the associated indexes \n",
    "        with open(path, 'r') as f:\n",
    "            ids = torch.LongTensor(tokens)\n",
    "            token = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "data = './wikitext-2-small/'\n",
    "corpus = Corpus(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383196\n",
      "19482\n",
      "19482\n",
      "torch.Size([275485])\n",
      "tensor([0, 1, 2, 3, 4, 1, 0])\n",
      "['<eos>', '=', 'Valkyria', 'Chronicles', 'III', '=', '<eos>']\n",
      "torch.Size([47945])\n",
      "tensor([    0,     1, 17642, 17643,     1,     0,     0])\n",
      "['<eos>', '=', 'Homarus', 'gammarus', '=', '<eos>', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(corpus.dictionary.total)\n",
    "print(len(corpus.dictionary.idx2word))\n",
    "print(len(corpus.dictionary.word2idx))\n",
    "\n",
    "print(corpus.train.shape)\n",
    "print(corpus.train[0:7])\n",
    "print([corpus.dictionary.idx2word[corpus.train[i]] for i in range(7)])\n",
    "\n",
    "print(corpus.valid.shape)\n",
    "print(corpus.valid[0:7])\n",
    "print([corpus.dictionary.idx2word[corpus.valid[i]] for i in range(7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have data under a very long list of indexes: the text is as one sequence.\n",
    "# The idea now is to create batches from this. Note that this is absolutely not the best\n",
    "# way to proceed with large quantities of data (where we'll try not to store huge tensors\n",
    "# in memory but read them from file as we go) !\n",
    "# Here, we are looking for simplicity and efficiency with regards to computation time.\n",
    "# That is why we will ignore sentence separations and treat the data as one long stream that\n",
    "# we will cut arbitrarily as we need.\n",
    "# With the alphabet being our data, we currently have the sequence:\n",
    "# [a b c d e f g h i j k l m n o p q r s t u v w x y z]\n",
    "# We want to reorganize it as independant batches that will be processed independantly by the model !\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get the 4 following sequences:\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘\n",
    "# with the last two elements being lost.\n",
    "# Again, these columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing.\n",
    "\n",
    "def batchify(data, batch_size, cuda = False):\n",
    "    # Cut the elements that are unnecessary\n",
    "    nbatch = data.size(0) // batch_size\n",
    "    data = data.narrow(0, 0, nbatch * batch_size)\n",
    "    # Reorganize the data\n",
    "    data = data.view(batch_size, -1).t().contiguous()\n",
    "    # If we can use a GPU, let's tranfer the tensor to it\n",
    "    if cuda:\n",
    "        data = data.cuda()\n",
    "    return data\n",
    "\n",
    "# get_batch subdivides the source data into chunks of the appropriate length.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a sequence length (seq_len) of 3, we'd get the following two variables:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# | b h n t | | c i o u │\n",
    "# └ c i o u ┘ └ d j p v ┘\n",
    "# The first variable contains the letters input to the network, while the second\n",
    "# contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
    "# Note that despite the name of the function, we are cutting the data in the\n",
    "# temporal dimension, since we already divided data into batches in the previous\n",
    "# function. \n",
    "\n",
    "def get_batch(source, i, seq_len, evaluation=False):\n",
    "    # Deal with the possibility that there's not enough data left for a full sequence\n",
    "    seq_len = min(seq_len, len(source) - 1 - i)\n",
    "    # Take the input data\n",
    "    data = source[i:i+seq_len]\n",
    "    # Shift by one for the target data\n",
    "    target = source[i+1:i+1+seq_len]\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2754, 100])\n",
      "torch.Size([11986, 4])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "eval_batch_size = 4\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,    10,    15,    91],\n",
      "        [    1,  3018,   735,    13],\n",
      "        [17642,   187,   766,   496]])\n",
      "tensor([[    1,  3018,   735,    13],\n",
      "        [17642,   187,   766,   496],\n",
      "        [17643,   827,   751,   131]])\n",
      "tensor([[17643,   827,   751,   131],\n",
      "        [    1,    19,  4659,  2200],\n",
      "        [    0,    17,  2466,    22]])\n",
      "tensor([[   1,   19, 4659, 2200],\n",
      "        [   0,   17, 2466,   22],\n",
      "        [   0, 3069,   39, 5521]])\n"
     ]
    }
   ],
   "source": [
    "input_words, target_words = get_batch(val_data, 0, 3)\n",
    "pp.pprint(input_words)\n",
    "pp.pprint(target_words)\n",
    "input_words, target_words = get_batch(val_data, 3, 3)\n",
    "pp.pprint(input_words)\n",
    "pp.pprint(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Cells in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0359, -0.0586,  0.2503]],\n",
      "\n",
      "        [[ 0.0035, -0.1231,  0.0139]],\n",
      "\n",
      "        [[ 0.1834, -0.0816, -0.1416]],\n",
      "\n",
      "        [[-0.0704, -0.3577, -0.2444]],\n",
      "\n",
      "        [[-0.0023, -0.3206, -0.1395]]], grad_fn=<StackBackward>)\n",
      "(tensor([[[-0.0023, -0.3206, -0.1395]]], grad_fn=<StackBackward>),\n",
      " tensor([[[-0.0048, -0.5989, -0.3442]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "# Create a toy example of LSTM: \n",
    "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
    "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
    "\n",
    "# LSTMs expect inputs having 3 dimensions:\n",
    "# - The first dimension is the temporal dimension, along which we (in our case) have the different words\n",
    "# - The second dimension is the batch dimension, along which we stack the independant batches\n",
    "# - The third dimension is the feature dimension, along which are the features of the vector representing the words\n",
    "\n",
    "# In our toy case, we have inputs and outputs containing 3 features (third dimension !)\n",
    "# We created a sequence of 5 different inputs (first dimension !)\n",
    "# We don't use batch (the second dimension will have one lement)\n",
    "\n",
    "# We need an initial hidden state, of the right sizes for dimension 2/3, but with only one temporal element:\n",
    "# Here, it is:\n",
    "hidden = (torch.randn(1, 1, 3),\n",
    "          torch.randn(1, 1, 3))\n",
    "# Why do we create a tuple of two tensors ? Because we use LSTMs: remember that they use two sets of weights,\n",
    "# and two hidden states (Hidden state, and Cell state).\n",
    "# If you don't remember, read: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "# If we used a classic RNN, we would simply have:\n",
    "# hidden = torch.randn(1, 1, 3)\n",
    "\n",
    "# The naive way of applying a lstm to inputs is to apply it one step at a time, and loop through the sequence\n",
    "for i in inputs:\n",
    "    # After each step, hidden contains the hidden states (remember, it's a tuple of two states).\n",
    "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
    "    \n",
    "# Alternatively, we can do the entire sequence all at once.\n",
    "# The first value returned by LSTM is all of the Hidden states throughout the sequence.\n",
    "# The second is just the most recent Hidden state and Cell state (you can compare the values)\n",
    "# The reason for this is that:\n",
    "# \"out\" will give you access to all hidden states in the sequence, for each temporal step\n",
    "# \"hidden\" will allow you to continue the sequence and backpropagate later, with another sequence\n",
    "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
    "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # Re-initialize\n",
    "out, hidden = lstm(inputs, hidden)\n",
    "pp.pprint(out)\n",
    "pp.pprint(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our own LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models are usually implemented as custom nn.Module subclass\n",
    "# We need to redefine the __init__ method, which creates the object\n",
    "# We also need to redefine the forward method, which transform the input into outputs\n",
    "# We can also add any method that we need: here, in order to initiate weights in the model\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Create a dropout object to use on layers for regularization\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        # Create an encoder - which is an embedding layer\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        # Create the LSTM layers - find out how to stack them !\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
    "        # (Note that the softmax application function will be applied out of the model)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        \n",
    "        # Initialize non-reccurent weights \n",
    "        self.init_weights()\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # Initialize the encoder and decoder weights with the uniform distribution\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize the hidden state and cell state to zero, with the right sizes\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
    "                weight.new_zeros(self.nlayers, batch_size, self.nhid))    \n",
    "\n",
    "    def forward(self, input, hidden, return_h=False):\n",
    "        # Process the input\n",
    "        emb = self.drop(self.encoder(input))   \n",
    "        \n",
    "        # Apply the LSTMs\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        # Decode into scores\n",
    "        output = self.drop(output)      \n",
    "        decoded = self.decoder(output)\n",
    "        return decoded, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# If you have Cuda installed and a GPU available\n",
    "cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    if not cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably choose cuda = True\")\n",
    "        \n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 200\n",
    "hidden_size = 200\n",
    "layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "vocab_size = len(corpus.dictionary)\n",
    "model = LSTMModel(vocab_size, embedding_size, hidden_size, layers, dropout).to(device)\n",
    "params = list(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 10.0\n",
    "optimizer = 'sgd'\n",
    "wdecay = 1.2e-6\n",
    "# For gradient clipping\n",
    "clip = 0.25\n",
    "\n",
    "if optimizer == 'sgd':\n",
    "    optim = torch.optim.SGD(params, lr=lr, weight_decay=wdecay)\n",
    "if optimizer == 'adam':\n",
    "    optim = torch.optim.Adam(params, lr=lr, weight_decay=wdecay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's think about gradient propagation:\n",
    "# We plan to keep the second ouput of the LSTM layer (the hidden/cell states) to initialize\n",
    "# the next call to LSTM. In this way, we can back-propagate the gradient for as long as we want.\n",
    "# However, this put a huge strain on the memory used by the model, since it implies retaining\n",
    "# a always-growing number of tensors of gradients in the cache.\n",
    "# We decide to not backpropagate through time beyond the current sequence ! \n",
    "# We use a specific function to cut the 'hidden/state cell' states from their previous dependencies\n",
    "# before using them to initialize the next call to the LSTM.\n",
    "# This is done with the .detach() function.\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other global parameters\n",
    "epochs = 10\n",
    "seq_len = 30\n",
    "log_interval = 10\n",
    "save = 'model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, seq_len):\n",
    "            data, targets = get_batch(data_source, i, seq_len)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data) * criterion(output.view(-1, vocab_size), targets.view(-1)).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
    "        data, targets = get_batch(train_data, i, seq_len)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        optim.zero_grad()\n",
    "        \n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
    "        optim.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // seq_len, lr,\n",
    "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    10/   91 batches | lr 10.00 | ms/batch 891.45 | loss  7.10 | ppl  1213.63\n",
      "| epoch   1 |    20/   91 batches | lr 10.00 | ms/batch 814.38 | loss  6.47 | ppl   648.23\n",
      "| epoch   1 |    30/   91 batches | lr 10.00 | ms/batch 826.88 | loss  6.46 | ppl   639.02\n",
      "| epoch   1 |    40/   91 batches | lr 10.00 | ms/batch 862.41 | loss  6.44 | ppl   625.17\n",
      "| epoch   1 |    50/   91 batches | lr 10.00 | ms/batch 868.89 | loss  6.39 | ppl   596.61\n",
      "| epoch   1 |    60/   91 batches | lr 10.00 | ms/batch 869.08 | loss  6.43 | ppl   622.77\n",
      "| epoch   1 |    70/   91 batches | lr 10.00 | ms/batch 889.35 | loss  6.39 | ppl   596.87\n",
      "| epoch   1 |    80/   91 batches | lr 10.00 | ms/batch 863.10 | loss  6.40 | ppl   602.38\n",
      "| epoch   1 |    90/   91 batches | lr 10.00 | ms/batch 863.47 | loss  6.41 | ppl   605.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 84.94s | valid loss  6.25 | valid ppl   515.44\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthieu/anaconda3/envs/TPs_NLP/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTMModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/matthieu/anaconda3/envs/TPs_NLP/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/matthieu/anaconda3/envs/TPs_NLP/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/matthieu/anaconda3/envs/TPs_NLP/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LSTM. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/matthieu/anaconda3/envs/TPs_NLP/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 |    10/   91 batches | lr 10.00 | ms/batch 953.08 | loss  7.00 | ppl  1093.45\n",
      "| epoch   2 |    20/   91 batches | lr 10.00 | ms/batch 872.14 | loss  6.38 | ppl   592.45\n",
      "| epoch   2 |    30/   91 batches | lr 10.00 | ms/batch 868.43 | loss  6.35 | ppl   573.16\n",
      "| epoch   2 |    40/   91 batches | lr 10.00 | ms/batch 860.24 | loss  6.35 | ppl   574.18\n",
      "| epoch   2 |    50/   91 batches | lr 10.00 | ms/batch 891.05 | loss  6.32 | ppl   556.25\n",
      "| epoch   2 |    60/   91 batches | lr 10.00 | ms/batch 854.66 | loss  6.36 | ppl   577.52\n",
      "| epoch   2 |    70/   91 batches | lr 10.00 | ms/batch 887.27 | loss  6.30 | ppl   541.90\n",
      "| epoch   2 |    80/   91 batches | lr 10.00 | ms/batch 892.64 | loss  6.31 | ppl   550.57\n",
      "| epoch   2 |    90/   91 batches | lr 10.00 | ms/batch 873.40 | loss  6.33 | ppl   563.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 87.07s | valid loss  6.19 | valid ppl   487.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    10/   91 batches | lr 10.00 | ms/batch 984.66 | loss  6.90 | ppl   992.67\n",
      "| epoch   3 |    20/   91 batches | lr 10.00 | ms/batch 902.55 | loss  6.27 | ppl   527.37\n",
      "| epoch   3 |    30/   91 batches | lr 10.00 | ms/batch 866.24 | loss  6.25 | ppl   517.86\n",
      "| epoch   3 |    40/   91 batches | lr 10.00 | ms/batch 900.21 | loss  6.28 | ppl   532.57\n",
      "| epoch   3 |    50/   91 batches | lr 10.00 | ms/batch 865.03 | loss  6.23 | ppl   508.10\n",
      "| epoch   3 |    60/   91 batches | lr 10.00 | ms/batch 862.56 | loss  6.26 | ppl   523.96\n",
      "| epoch   3 |    70/   91 batches | lr 10.00 | ms/batch 872.31 | loss  6.19 | ppl   489.97\n",
      "| epoch   3 |    80/   91 batches | lr 10.00 | ms/batch 856.28 | loss  6.22 | ppl   505.18\n",
      "| epoch   3 |    90/   91 batches | lr 10.00 | ms/batch 868.51 | loss  6.24 | ppl   513.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 87.73s | valid loss  6.21 | valid ppl   497.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    10/   91 batches | lr 2.50 | ms/batch 1095.94 | loss  6.81 | ppl   902.39\n",
      "| epoch   4 |    20/   91 batches | lr 2.50 | ms/batch 898.55 | loss  6.20 | ppl   494.91\n",
      "| epoch   4 |    30/   91 batches | lr 2.50 | ms/batch 859.57 | loss  6.16 | ppl   474.71\n",
      "| epoch   4 |    40/   91 batches | lr 2.50 | ms/batch 881.30 | loss  6.17 | ppl   476.80\n",
      "| epoch   4 |    50/   91 batches | lr 2.50 | ms/batch 884.08 | loss  6.15 | ppl   467.11\n",
      "| epoch   4 |    60/   91 batches | lr 2.50 | ms/batch 863.03 | loss  6.18 | ppl   481.47\n",
      "| epoch   4 |    70/   91 batches | lr 2.50 | ms/batch 872.02 | loss  6.12 | ppl   456.01\n",
      "| epoch   4 |    80/   91 batches | lr 2.50 | ms/batch 876.47 | loss  6.15 | ppl   466.84\n",
      "| epoch   4 |    90/   91 batches | lr 2.50 | ms/batch 877.73 | loss  6.16 | ppl   473.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 88.59s | valid loss  6.11 | valid ppl   448.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    10/   91 batches | lr 2.50 | ms/batch 953.37 | loss  6.70 | ppl   810.53\n",
      "| epoch   5 |    20/   91 batches | lr 2.50 | ms/batch 880.34 | loss  6.12 | ppl   457.05\n",
      "| epoch   5 |    30/   91 batches | lr 2.50 | ms/batch 896.79 | loss  6.09 | ppl   442.57\n",
      "| epoch   5 |    40/   91 batches | lr 2.50 | ms/batch 896.23 | loss  6.11 | ppl   449.89\n",
      "| epoch   5 |    50/   91 batches | lr 2.50 | ms/batch 887.62 | loss  6.06 | ppl   426.74\n",
      "| epoch   5 |    60/   91 batches | lr 2.50 | ms/batch 893.88 | loss  6.08 | ppl   435.97\n",
      "| epoch   5 |    70/   91 batches | lr 2.50 | ms/batch 871.66 | loss  6.04 | ppl   420.07\n",
      "| epoch   5 |    80/   91 batches | lr 2.50 | ms/batch 887.36 | loss  6.06 | ppl   428.91\n",
      "| epoch   5 |    90/   91 batches | lr 2.50 | ms/batch 871.79 | loss  6.09 | ppl   441.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 87.83s | valid loss  6.05 | valid ppl   425.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |    10/   91 batches | lr 2.50 | ms/batch 964.99 | loss  6.63 | ppl   756.38\n",
      "| epoch   6 |    20/   91 batches | lr 2.50 | ms/batch 871.58 | loss  6.04 | ppl   418.58\n",
      "| epoch   6 |    30/   91 batches | lr 2.50 | ms/batch 887.29 | loss  6.00 | ppl   401.94\n",
      "| epoch   6 |    40/   91 batches | lr 2.50 | ms/batch 863.94 | loss  6.03 | ppl   417.73\n",
      "| epoch   6 |    50/   91 batches | lr 2.50 | ms/batch 859.41 | loss  6.00 | ppl   402.08\n",
      "| epoch   6 |    60/   91 batches | lr 2.50 | ms/batch 880.16 | loss  6.02 | ppl   413.17\n",
      "| epoch   6 |    70/   91 batches | lr 2.50 | ms/batch 873.08 | loss  5.94 | ppl   380.84\n",
      "| epoch   6 |    80/   91 batches | lr 2.50 | ms/batch 887.74 | loss  6.00 | ppl   403.65\n",
      "| epoch   6 |    90/   91 batches | lr 2.50 | ms/batch 880.70 | loss  6.02 | ppl   410.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 87.12s | valid loss  6.02 | valid ppl   410.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |    10/   91 batches | lr 2.50 | ms/batch 963.78 | loss  6.55 | ppl   698.62\n",
      "| epoch   7 |    20/   91 batches | lr 2.50 | ms/batch 876.11 | loss  5.95 | ppl   384.20\n",
      "| epoch   7 |    30/   91 batches | lr 2.50 | ms/batch 874.01 | loss  5.94 | ppl   380.84\n",
      "| epoch   7 |    40/   91 batches | lr 2.50 | ms/batch 894.47 | loss  5.97 | ppl   389.89\n",
      "| epoch   7 |    50/   91 batches | lr 2.50 | ms/batch 871.79 | loss  5.92 | ppl   372.15\n",
      "| epoch   7 |    60/   91 batches | lr 2.50 | ms/batch 886.89 | loss  5.94 | ppl   380.36\n",
      "| epoch   7 |    70/   91 batches | lr 2.50 | ms/batch 872.78 | loss  5.90 | ppl   366.13\n",
      "| epoch   7 |    80/   91 batches | lr 2.50 | ms/batch 885.61 | loss  5.92 | ppl   370.95\n",
      "| epoch   7 |    90/   91 batches | lr 2.50 | ms/batch 879.11 | loss  5.96 | ppl   386.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 87.43s | valid loss  5.99 | valid ppl   399.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |    10/   91 batches | lr 2.50 | ms/batch 956.44 | loss  6.48 | ppl   654.07\n",
      "| epoch   8 |    20/   91 batches | lr 2.50 | ms/batch 893.82 | loss  5.88 | ppl   358.20\n",
      "| epoch   8 |    30/   91 batches | lr 2.50 | ms/batch 901.81 | loss  5.87 | ppl   353.59\n",
      "| epoch   8 |    40/   91 batches | lr 2.50 | ms/batch 867.38 | loss  5.91 | ppl   367.21\n",
      "| epoch   8 |    50/   91 batches | lr 2.50 | ms/batch 865.20 | loss  5.85 | ppl   347.35\n",
      "| epoch   8 |    60/   91 batches | lr 2.50 | ms/batch 871.29 | loss  5.87 | ppl   353.65\n",
      "| epoch   8 |    70/   91 batches | lr 2.50 | ms/batch 866.99 | loss  5.84 | ppl   343.57\n",
      "| epoch   8 |    80/   91 batches | lr 2.50 | ms/batch 878.91 | loss  5.85 | ppl   348.32\n",
      "| epoch   8 |    90/   91 batches | lr 2.50 | ms/batch 893.96 | loss  5.89 | ppl   361.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 87.36s | valid loss  5.92 | valid ppl   373.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |    10/   91 batches | lr 2.50 | ms/batch 975.76 | loss  6.42 | ppl   613.60\n",
      "| epoch   9 |    20/   91 batches | lr 2.50 | ms/batch 872.90 | loss  5.81 | ppl   333.24\n",
      "| epoch   9 |    30/   91 batches | lr 2.50 | ms/batch 876.47 | loss  5.82 | ppl   338.32\n",
      "| epoch   9 |    40/   91 batches | lr 2.50 | ms/batch 874.25 | loss  5.83 | ppl   340.19\n",
      "| epoch   9 |    50/   91 batches | lr 2.50 | ms/batch 1058.59 | loss  5.79 | ppl   328.49\n",
      "| epoch   9 |    60/   91 batches | lr 2.50 | ms/batch 897.72 | loss  5.81 | ppl   334.73\n",
      "| epoch   9 |    70/   91 batches | lr 2.50 | ms/batch 889.58 | loss  5.77 | ppl   320.48\n",
      "| epoch   9 |    80/   91 batches | lr 2.50 | ms/batch 869.75 | loss  5.79 | ppl   328.29\n",
      "| epoch   9 |    90/   91 batches | lr 2.50 | ms/batch 886.83 | loss  5.81 | ppl   334.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 89.38s | valid loss  5.90 | valid ppl   363.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |    10/   91 batches | lr 2.50 | ms/batch 953.72 | loss  6.35 | ppl   569.92\n",
      "| epoch  10 |    20/   91 batches | lr 2.50 | ms/batch 889.94 | loss  5.75 | ppl   315.50\n",
      "| epoch  10 |    30/   91 batches | lr 2.50 | ms/batch 896.64 | loss  5.76 | ppl   317.48\n",
      "| epoch  10 |    40/   91 batches | lr 2.50 | ms/batch 876.19 | loss  5.77 | ppl   321.24\n",
      "| epoch  10 |    50/   91 batches | lr 2.50 | ms/batch 865.93 | loss  5.73 | ppl   309.40\n",
      "| epoch  10 |    60/   91 batches | lr 2.50 | ms/batch 884.51 | loss  5.75 | ppl   313.00\n",
      "| epoch  10 |    70/   91 batches | lr 2.50 | ms/batch 963.31 | loss  5.70 | ppl   299.91\n",
      "| epoch  10 |    80/   91 batches | lr 2.50 | ms/batch 912.85 | loss  5.72 | ppl   303.71\n",
      "| epoch  10 |    90/   91 batches | lr 2.50 | ms/batch 869.23 | loss  5.77 | ppl   321.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 88.73s | valid loss  5.87 | valid ppl   354.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.93 | test ppl   375.74\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Loop over epochs.\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(save, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(save, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    # after load the rnn params are not a continuous chunk of memory\n",
    "    # this makes them a continuous chunk, and will speed up forward pass\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
