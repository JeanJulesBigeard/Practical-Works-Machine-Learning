{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeu du 421"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook montre quelques algorithmes d'apprentissage par renforcement appliqués au jeu du [421](https://fr.wikipedia.org/wiki/421_(jeu)) :\n",
    "* Value Iteration\n",
    "* Policy Iteration\n",
    "* SARSA\n",
    "* Q-learning\n",
    "\n",
    "L'objectif est ici de maximiser son score moyen en trois coups au plus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valeurs des dés\n",
    "dice = np.arange(1,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de lancers restants\n",
    "throws = np.arange(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores():\n",
    "    scores = {(4,2,1): 11, (1,1,1): 7, (2,2,1): 0}\n",
    "    scores.update({(d,1,1): d for d in dice if d > 1})\n",
    "    scores.update({(d,d,d): d for d in dice if d > 1})\n",
    "    scores.update({(d,d - 1,d - 2): 2 for d in dice if d > 2})\n",
    "    specials = list(scores.keys())\n",
    "    scores.update({(d,e,f): 1 for d in dice for e in dice if e <= d for f in dice if f <= e and (d,e,f) not in specials})\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les dés que l'on a\n",
    "states = list(scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les dés que l'on relance\n",
    "actions = [(a,b,c) for a in range(2) for b in range(2) for c in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state):\n",
    "    return scores[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_prob(state, action):\n",
    "    prob = {}\n",
    "    nb = np.sum(action)\n",
    "    for i in range(6**nb):\n",
    "        new_state = np.array(state)\n",
    "        die = i\n",
    "        for j in range(3):\n",
    "            if action[j]:\n",
    "                new_state[j] = die % 6 + 1\n",
    "                die = die // 6\n",
    "        new_state = tuple(sorted(new_state,reverse = True))\n",
    "        if new_state in prob:\n",
    "            prob[new_state] += 1 / 6**nb\n",
    "        else:\n",
    "            prob[new_state] = 1 / 6**nb\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(tol = 1e-2, max_iter = 50):\n",
    "    '''\n",
    "    Returns the value function\n",
    "        (state, throw) = état + nombre de lancers restants\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tol: float\n",
    "        tolerance for convergence\n",
    "    max_iter: int\n",
    "        max number of iterations\n",
    "    '''\n",
    "    V = {(state, throw): 0 for state in states for throw in throws}\n",
    "    error = np.inf\n",
    "    iteration = 0\n",
    "    while error > tol:\n",
    "        for state, throw in V:\n",
    "            if throw == 0:\n",
    "                V[(state, throw)] = get_reward(state)\n",
    "            else:\n",
    "                max_value = 0\n",
    "                for action in actions:\n",
    "                    value = 0\n",
    "                    prob = transition_prob(state, action)\n",
    "                    for s in prob:\n",
    "                        value += prob[s] * V[(s,throw - 1)]\n",
    "                    if value > max_value:\n",
    "                        max_value = value\n",
    "                error_ = np.abs(max_value - V[(state, throw)])\n",
    "                if error_ > error:\n",
    "                    error = error_\n",
    "                V[(state, throw)] = max_value\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_policy(value):\n",
    "    policy = {}\n",
    "    for state, throw in value:\n",
    "        if throw > 0:\n",
    "            v_max = 0\n",
    "            for action in actions:\n",
    "                v = 0\n",
    "                prob = transition_prob(state, action)\n",
    "                for s in prob:\n",
    "                    v += prob[s] * value[(s,throw - 1)]\n",
    "                if v > v_max:\n",
    "                    v_max = v\n",
    "                    best_action = action\n",
    "            policy[(state, throw)] = best_action\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = best_policy(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, throw, policy):\n",
    "    if throw == 0:\n",
    "        return (0,0,0)\n",
    "    else:\n",
    "        return policy[(state, throw)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_state():\n",
    "    a = np.random.choice(6) + 1\n",
    "    b = np.random.choice(6) + 1\n",
    "    c = np.random.choice(6) + 1\n",
    "    return tuple(sorted((a,b,c),reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(state, action):\n",
    "    prob = transition_prob(state,action)\n",
    "    i = np.random.choice(np.arange(len(prob)), p = list(prob.values()))\n",
    "    return list(prob.keys())[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_policy(policy, runs = 1000):\n",
    "    score = 0\n",
    "    for i in range(runs):\n",
    "        # play the game\n",
    "        throw = 2\n",
    "        state = random_state()\n",
    "        action = get_action(state, throw, policy)\n",
    "        sequence = []\n",
    "        while action != (0,0,0):\n",
    "            throw -= 1\n",
    "            state = move(state, action)\n",
    "            action = get_action(state, throw, policy)\n",
    "        score += get_reward(state)\n",
    "    return score / runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.827"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_policy():\n",
    "    policy = {(state, throw): actions[np.random.choice(len(actions))] for state in states for throw in throws}\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, tol = 1e-2, max_iter = 50):\n",
    "    '''\n",
    "    tol: float\n",
    "        tolerance for convergence\n",
    "    max_iter: float\n",
    "        max number of iterations\n",
    "    '''\n",
    "    V = {(state, throw): 0 for state in states for throw in throws}\n",
    "    error = np.inf\n",
    "    iteration = 0\n",
    "    while error > tol:\n",
    "        for state, throw in V:\n",
    "            if throw == 0:\n",
    "                V[(state, throw)] = get_reward(state)\n",
    "            else:\n",
    "                action = policy[(state, throw)]\n",
    "                new_throw = throw - 1\n",
    "                new_state = move(state, action)\n",
    "                if action == (0,0,0):\n",
    "                    V[(state, throw)] = get_reward(state)\n",
    "                else:\n",
    "                    value = get_reward(new_state) + V[(new_state, new_throw)]\n",
    "                    error_ = np.abs(value - V[(state, throw)])\n",
    "                    if error_ > error:\n",
    "                        error = error_\n",
    "                    V[(state, throw)] = value\n",
    "        iteration += 1\n",
    "        if iteration >= max_iter:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = init_random_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_evaluation(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = best_policy(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_evaluation(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = best_policy(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.964"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the transition probabilities are supposed to be unknown and need to be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_explore(state, throw, policy, eps = 0.1):\n",
    "    '''\n",
    "    eps: float\n",
    "        Parameter of the epsilon-greedy policy\n",
    "        Controls the exploration (eps = 0 means no exploration)\n",
    "    '''\n",
    "    if throw == 0:\n",
    "        return (0,0,0)\n",
    "    elif np.random.random() < eps:\n",
    "        return actions[np.random.choice(len(actions))]\n",
    "    else:\n",
    "        return policy[(state, throw)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(init_policy = 'random', alpha = 0.9, steps = 10**5, verbose = True, batch = 10**4):\n",
    "    if init_policy == 'random':\n",
    "        policy = init_random_policy()\n",
    "    elif init_policy == 'all':\n",
    "        policy = {(state, throw): (1,1,1)  for state in states for throw in throws}\n",
    "    elif init_policy == 'none':\n",
    "        policy = {(state, throw): (0,0,0)  for state in states for throw in throws}\n",
    "    Q = {(state, throw, action): 0 for state in states for throw in throws for action in actions}\n",
    "    for t in range(steps):\n",
    "        if verbose: \n",
    "            if t%batch == 0:\n",
    "                print('Batch ',str(t // batch + 1),' over ',str(steps // batch))\n",
    "        # play the game\n",
    "        state = random_state()\n",
    "        throw = 2\n",
    "        action = get_action_explore(state, throw, policy)\n",
    "        sequence = []\n",
    "        while action != (0,0,0):\n",
    "            sequence.append((state, throw))\n",
    "            new_throw = throw - 1\n",
    "            new_state = move(state, action)\n",
    "            new_action = get_action_explore(new_state, new_throw, policy)\n",
    "            Q[(state, throw, action)] += alpha * (Q[(new_state, new_throw, new_action)] \n",
    "                                                  - Q[(state, throw, action)]) \n",
    "            throw = new_throw\n",
    "            state = new_state\n",
    "            action = new_action\n",
    "        sequence.append((state, throw))\n",
    "        Q[(state, throw, action)] += alpha * (get_reward(state) - Q[(state, throw, action)]) \n",
    "        # update policy\n",
    "        for state, throw in sequence:\n",
    "            qvalues = {a: Q[(state, throw, a)] for a in actions}\n",
    "            policy[(state, throw)] = max(qvalues, key = qvalues.get)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1  over  10\n",
      "Batch  2  over  10\n",
      "Batch  3  over  10\n",
      "Batch  4  over  10\n",
      "Batch  5  over  10\n",
      "Batch  6  over  10\n",
      "Batch  7  over  10\n",
      "Batch  8  over  10\n",
      "Batch  9  over  10\n",
      "Batch  10  over  10\n"
     ]
    }
   ],
   "source": [
    "sarsa_policy = sarsa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.951"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_policy(sarsa_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlearning(init_policy = 'random', alpha = 0.9, steps = 10**5, verbose = True, batch = 10**4):\n",
    "    if init_policy == 'random':\n",
    "        policy = init_random_policy()\n",
    "    elif init_policy == 'all':\n",
    "        policy = {(state, throw): (1,1,1)  for state in states for throw in throws}\n",
    "    elif init_policy == 'none':\n",
    "        policy = {(state, throw): (0,0,0)  for state in states for throw in throws}\n",
    "    Q = {(state, throw, action): 0 for state in states for throw in throws for action in actions}\n",
    "    for t in range(steps):\n",
    "        if verbose:\n",
    "            if t%batch == 0:\n",
    "                print('Batch ',str(t // batch + 1),' over ',str(steps // batch))\n",
    "        # play the game\n",
    "        state = random_state()\n",
    "        throw = 2\n",
    "        action = get_action_explore(state, throw, policy)\n",
    "        sequence = []\n",
    "        while action != (0,0,0):\n",
    "            sequence.append((state, throw))\n",
    "            new_throw = throw - 1\n",
    "            new_state = move(state, action)\n",
    "            qvalues = [Q[(new_state, new_throw, a)] for a in actions]\n",
    "            Q[(state, throw, action)] += alpha * (np.max(np.array(qvalues)) - \n",
    "                                                  Q[(state, throw, action)]) \n",
    "            throw = new_throw\n",
    "            state = new_state\n",
    "            action = get_action_explore(state, throw, policy)\n",
    "        sequence.append((state, throw))\n",
    "        Q[(state, throw, action)] += alpha * (get_reward(state) - Q[(state, throw, action)]) \n",
    "        # update policy\n",
    "        for state, throw in sequence:\n",
    "            qvalues = {a: Q[(state, throw, a)] for a in actions}\n",
    "            policy[(state, throw)] = max(qvalues, key = qvalues.get)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1  over  10\n",
      "Batch  2  over  10\n",
      "Batch  3  over  10\n",
      "Batch  4  over  10\n",
      "Batch  5  over  10\n",
      "Batch  6  over  10\n",
      "Batch  7  over  10\n",
      "Batch  8  over  10\n",
      "Batch  9  over  10\n",
      "Batch  10  over  10\n"
     ]
    }
   ],
   "source": [
    "qlearning_policy = qlearning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.867"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_policy(qlearning_policy)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
